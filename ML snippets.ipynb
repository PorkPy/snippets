{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da4f74b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T23:27:42.881275Z",
     "start_time": "2022-11-13T23:27:41.282995Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (StandardScaler, \n",
    "                                   RobustScaler, \n",
    "                                   PolynomialFeatures)\n",
    "\n",
    "from sklearn.pipeline import (make_pipeline, \n",
    "                             make_union, \n",
    "                             FeatureUnion)\n",
    "\n",
    "from tpot.builtins import StackingEstimator, OneHotEncoder\n",
    "\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (VotingClassifier, \n",
    "                             StackingClassifier,\n",
    "                             BaggingClassifier,\n",
    "                             RandomForestClassifier, \n",
    "                             AdaBoostClassifier, \n",
    "                             ExtraTreesClassifier, \n",
    "                             GradientBoostingClassifier)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import clone \n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ca087",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def pay_method_models(pay_method):   \n",
    "    \n",
    "    print(\"Payment Method: \", pay_method)\n",
    "    \n",
    "    if pay_method == 'DDR':\n",
    "        clf = ExtraTreesClassifier(bootstrap=False, \n",
    "                                     criterion=\"gini\", \n",
    "                                     max_features=0.1, \n",
    "                                     min_samples_leaf=1, \n",
    "                                     min_samples_split=3, \n",
    "                                     n_estimators=100)\n",
    "\n",
    "    elif pay_method == 'DEO': \n",
    "        clf = make_pipeline(\n",
    "                StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=False, \n",
    "                                                                 criterion=\"entropy\", \n",
    "                                                                 max_features=0.45, \n",
    "                                                                 min_samples_leaf=2, \n",
    "                                                                 min_samples_split=15, \n",
    "                                                                 n_estimators=100)),\n",
    "                StackingEstimator(estimator=LinearSVC(C=20.0, \n",
    "                                                      dual=False, \n",
    "                                                      loss=\"squared_hinge\", \n",
    "                                                      penalty=\"l2\", tol=1e-05)),\n",
    "                OneHotEncoder(minimum_fraction=0.1, \n",
    "                              sparse=False, \n",
    "                              threshold=10),\n",
    "                GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                           max_depth=8, \n",
    "                                           max_features=0.4, \n",
    "                                           min_samples_leaf=16, \n",
    "                                           min_samples_split=7, \n",
    "                                           n_estimators=100, \n",
    "                                           subsample=0.7000000000000001)\n",
    "            )\n",
    "\n",
    "    elif pay_method == 'SDO':\n",
    "        clf = make_pipeline(\n",
    "                PCA(iterated_power=7, \n",
    "                    svd_solver=\"randomized\"),\n",
    "                ExtraTreesClassifier(bootstrap=False, \n",
    "                                     criterion=\"entropy\", \n",
    "                                     max_features=0.8, \n",
    "                                     min_samples_leaf=1, \n",
    "                                     min_samples_split=4, \n",
    "                                     n_estimators=100)\n",
    "            )\n",
    "\n",
    "    elif pay_method == 'DFB': \n",
    "        clf = make_pipeline(\n",
    "                PCA(iterated_power=9, \n",
    "                    svd_solver=\"randomized\"),\n",
    "                GradientBoostingClassifier(learning_rate=0.5, \n",
    "                                           max_depth=9, \n",
    "                                           max_features=0.9500000000000001, \n",
    "                                           min_samples_leaf=13, \n",
    "                                           min_samples_split=7, \n",
    "                                           n_estimators=100, \n",
    "                                           subsample=0.7000000000000001)\n",
    "            )\n",
    "\n",
    "    elif pay_method == 'VSO': \n",
    "        clf = make_pipeline(\n",
    "                FastICA(tol=0.05),\n",
    "                StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                                                       max_depth=9, \n",
    "                                                                       max_features=0.2, \n",
    "                                                                       min_samples_leaf=4, \n",
    "                                                                       min_samples_split=20, \n",
    "                                                                       n_estimators=100, \n",
    "                                                                       subsample=1.0)),\n",
    "                GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                           max_depth=9, \n",
    "                                           max_features=0.1, \n",
    "                                           min_samples_leaf=3, \n",
    "                                           min_samples_split=15, \n",
    "                                           n_estimators=100, \n",
    "                                           subsample=0.5)\n",
    "            )\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f048a55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T23:32:36.591388Z",
     "start_time": "2022-11-13T23:32:36.580458Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(grid_search=False):\n",
    "    \n",
    "    XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                  colsample_bynode=1, colsample_bytree=1.0,\n",
    "                  enable_categorical=False, eta=0.4, gamma=0.5, gpu_id=-1,\n",
    "                  importance_type=None, interaction_constraints='',\n",
    "                  learning_rate=0.400000006, max_delta_step=0, max_depth=6,\n",
    "                  min_child_weight=1,monotone_constraints='()',\n",
    "                  n_estimators=100, n_jobs=12, num_parallel_tree=4,\n",
    "                  predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "                  #scale_pos_weight=1, \n",
    "                        subsample=0.8, tree_method='exact',\n",
    "                  validate_parameters=1, verbosity=None, num_class=1)\n",
    "\n",
    "    ETC = ExtraTreesClassifier(bootstrap=True, \n",
    "                                 criterion=\"gini\", \n",
    "                                 max_features=0.4, \n",
    "                                 min_samples_leaf=5, \n",
    "                                 min_samples_split=16, \n",
    "                                 n_estimators=100)\n",
    "\n",
    "\n",
    "    ETC2 = ExtraTreesClassifier(bootstrap=True, \n",
    "                                 criterion=\"entropy\", \n",
    "                                 max_features=0.8, \n",
    "                                 min_samples_leaf=2, \n",
    "                                 min_samples_split=2, \n",
    "                                 n_estimators=100)\n",
    "\n",
    "    RFC = RandomForestClassifier(bootstrap=False, \n",
    "                                 criterion=\"gini\",\n",
    "                                 max_depth=24,#24\n",
    "                                 max_features=0.25, #0.25\n",
    "                                 min_samples_leaf=1, \n",
    "                                 min_samples_split=4, \n",
    "                                 n_estimators=1000)#50\n",
    "\n",
    "\n",
    "    GBC = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                   max_depth=8, \n",
    "                                   max_features=0.15000000000000002, \n",
    "                                   min_samples_leaf=16, \n",
    "                                   min_samples_split=10, \n",
    "                                   n_estimators=100, \n",
    "                                   subsample=0.25)\n",
    "\n",
    "    svc = SVC()\n",
    "    ABC = AdaBoostClassifier()\n",
    "    DTC = DecisionTreeClassifier()\n",
    "    KNC = KNeighborsClassifier()\n",
    "    LR  = LogisticRegression()\n",
    "    MNB = MultinomialNB()\n",
    "    CAT = CatBoostClassifier(silent=True)\n",
    "    \n",
    "\n",
    "    \"\"\"  Transformers. \"\"\"\n",
    "    Column_Transformer = make_column_transformer(\n",
    "                          \n",
    "                          (OneHotEncoder(),\n",
    "                           make_column_selector(dtype_include=object))) \n",
    "    \n",
    "    Transformers = [ ('col_trans', Column_Transformer),\n",
    "                    #('ohe', OneHotEncoder()),\n",
    "                    #('Scaler', RobustScaler()),\n",
    "                    #('kernel_pca', KernelPCA()),\n",
    "                    #('reduce_dim', PCA(.98)),\n",
    "                    #('KBest', SelectKBest(chi2, k=35)), #.fit_transform(X, y)),\n",
    "                    #('FEATURE', PolynomialFeatures(degree=2,include_bias=False))\n",
    "    ]\n",
    "    ## Combine transformed features\n",
    "    Transformer_Union = FeatureUnion(Transformers)\n",
    "    \"\"\"********************************************\"\"\"\n",
    "\n",
    "    \"\"\" Estimators \"\"\"\n",
    "    # Create Base Learners for stacking. XGB+RFC+CAT = 0.948\n",
    "    base_learners = [\n",
    "                      ('XGB', XGB), # 0.941 # 0.945\n",
    "                      #('ETC', ETC), # 0.930 # 0.938\n",
    "                      ('RFC', RFC), # 0.943 # 0.947\n",
    "                      #('DTC', DTC), # 0.916 # 0.914\n",
    "                      #('GBC', GBC), # 0.932 # 0.938\n",
    "                      #('SVC', svc), # 0.830\n",
    "                      #('KNC', KNC), # 0.848\n",
    "                      #('ABC', ABC), # 0.906\n",
    "                      #('LR', LR),   # 0873\n",
    "                      #('MNB', MNB),  # 0.812\n",
    "                      ('CAT', CAT),  # ----- # 0.945\n",
    "    ]\n",
    "\n",
    "    # # Initialize Stacking Classifier with the Meta Learner\n",
    "    stack = StackingClassifier(estimators      = base_learners, \n",
    "                               final_estimator = LR,\n",
    "                              verbose=2) # Base=XGB=0.941. XGB+RFC+CAT = 0.950\n",
    "    ## Voting Classifier\n",
    "    vote = VotingClassifier(estimators = base_learners, #XGB=0.941\n",
    "                            voting     ='soft');\n",
    "    ## Bagging Classifier\n",
    "    bag = BaggingClassifier(base_estimator = CAT, # XGB=0.940 RFC=0.941\n",
    "                            n_estimators   = 100, # maybe n_estimatoers is too small. \n",
    "                            random_state   = 0)\n",
    "    \"\"\"****************************************\"\"\"\n",
    "\n",
    "    \"\"\" Clf PIPELINE \"\"\"\n",
    "    clf = make_pipeline(None, RFC); ## use * infront of Transformers to unpack list when not using union.\n",
    "    \n",
    "    \"\"\"******************************************\"\"\"\n",
    "\n",
    "    \"\"\"  GRID SEARCH. \"\"\"\n",
    "    if grid_search:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, df = get_data(_SMOTE=_SMOTE)\n",
    "        clf = RandomizedSearchCV(estimator = clf, param_distributions = get_gridsearch(), \n",
    "                                                   n_iter = 100, #100\n",
    "                                                   cv = 3,#3 \n",
    "                                                   verbose=2, \n",
    "                                                   random_state=42, \n",
    "                                                   n_jobs = -1)\n",
    "        ## Retrain on best estimator \n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"best_estimator_ \\t\\n:\", clf.best_estimator_)\n",
    "        print(\"best_score_ \\t\\n\",      clf.best_score_)\n",
    "        print(\"best_params_ \\t\\n\",     clf.best_params_)\n",
    "        clf = clf.best_estimator_\n",
    "        print(clf) \n",
    "    \"\"\"************************************************************\"\"\"\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa059615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T23:33:59.949347Z",
     "start_time": "2022-11-13T23:33:59.945510Z"
    }
   },
   "outputs": [],
   "source": [
    "### Randomised Grid Search\n",
    "\n",
    "def get_gridsearch():\n",
    "    \n",
    "    \"\"\"\n",
    "        Params example for nested classifiers.\n",
    "        Use 'clf.get_params()'' to get a list of the different parameters available.\n",
    "        \n",
    "        params = [{'votingclassifier__XGB__learning_rate':    [0.1, 0.01, 0.001, 0.0001],\n",
    "                   'votingclassifier__XGB__min_child_weight': [1,2,3,4],\n",
    "                   'votingclassifier__XGB__subsample':        [0.2, 0.4, 0.6, 0.8]}]\n",
    "        \"\"\"\n",
    "    \n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 1, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(1, 200, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'randomforestclassifier__n_estimators': n_estimators,\n",
    "                   'randomforestclassifier__max_features': max_features,\n",
    "                   'randomforestclassifier__max_depth': max_depth,\n",
    "                   'randomforestclassifier__min_samples_split': min_samples_split,\n",
    "                   'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n",
    "                   'randomforestclassifier__bootstrap': bootstrap}\n",
    "    #pprint(random_grid)\n",
    "\n",
    "    return random_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(grid_search=False, _SMOTE=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, df = get_data(_SMOTE=_SMOTE)\n",
    "\n",
    "    \"\"\" get model. \"\"\"\n",
    "    clf = build_model(grid_search)\n",
    "\n",
    "    \"\"\" Train. \"\"\"\n",
    "    np.random.seed(4)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    \"\"\" Test. \"\"\"\n",
    "    preds = clf.predict(X_test)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    print()\n",
    "    print(\"Train Score \\t\", train_score)\n",
    "    print(\"Test Score \\t\", test_score)\n",
    "    ## of all the cases predicted positive, how many were actually positive?\n",
    "    print(\"precision_score \", precision_score(y_test, preds))\n",
    "    ## of all the positive cases, how may did the model identify?\n",
    "    print(\"recall_score \\t\", recall_score(y_test, preds))\n",
    "        \n",
    "    return train_score, test_score, preds, clf, X_train, y_train, X_test, y_test, df\n",
    "    # 0.94644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\n",
    "data = data.dropna()\n",
    "\n",
    "#display(data)\n",
    "data2 = data.drop('Name', axis=1)\n",
    "data3 = data2 *1\n",
    "data3 = data3.dropna()\n",
    "## split Cabin into deck and side. \n",
    "data3['deck'] = [x[0] for x in data3['Cabin']]\n",
    "data3['side'] = [x[-1] for x in data3['Cabin']]\n",
    "data3['side'].replace(['P', 'S'], [0, 1], inplace=True)\n",
    "data3['VIP'].replace([0, 1], [0, 1], inplace=True)\n",
    "data3['CryoSleep'].replace([0, 1], [0, 1], inplace=True)\n",
    "\n",
    "data3.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "data3['PassengerId'] = data3['PassengerId'].astype(str)\n",
    "## remove individual identifyer from end of ID.\n",
    "data3['PassengerId'] = [x[0:-1] for x in data3['PassengerId']]\n",
    "\n",
    "data3['PassengerId'] = [x.replace('_','') for x in data3['PassengerId']]\n",
    "\n",
    "data3['PassengerId'] = data3['PassengerId'].astype(int)\n",
    "data3['age_group'] = pd.cut(data3['Age'], \n",
    "                            bins=[-1, 10, 19, 29, 49, 80],\n",
    "                           labels=[10, 19, 29, 49, 80]).astype(int)\n",
    "data3['FoodCourt_group'] = pd.cut(data3['FoodCourt'], \n",
    "                            bins=[-1, 0, 10, 100, 1000, 30000],\n",
    "                           labels=[0, 10, 100, 1000, 30000]).astype(int)\n",
    "\n",
    "data3['RoomService_group'] = pd.cut(data3['RoomService'], \n",
    "                            bins=[-1, 0, 10, 100, 1000, 10000],\n",
    "                           labels=[0, 10, 100, 1000, 10000]).astype(int)\n",
    "\n",
    "data3['ShoppingMall_group'] = pd.cut(data3['ShoppingMall'], \n",
    "                            bins=[-1, 0, 10, 100, 1000, 15000],\n",
    "                           labels=[0, 10, 100, 1000, 15000]).astype(int)\n",
    "\n",
    "data3['Spa_group'] = pd.cut(data3['Spa'], \n",
    "                            bins=[-1, 0, 10, 100, 1000, 30000],\n",
    "                           labels=[0, 10, 100, 1000, 30000]).astype(int)\n",
    "\n",
    "data3['VRDeck_group'] = pd.cut(data3['VRDeck'], \n",
    "                            bins=[-1, 0, 10, 100, 1000, 21000],\n",
    "                           labels=[0, 10, 100, 1000, 21000]).astype(int)\n",
    "\n",
    "\n",
    "bob = pd.get_dummies(data3[ ['HomePlanet','Destination', 'deck']])\n",
    "\n",
    "data3 = data3.drop('HomePlanet', axis=1)\n",
    "data3 = data3.drop('Destination', axis=1)\n",
    "data3 = data3.drop('deck', axis=1)\n",
    "data3 = data3.drop('Age', axis=1)\n",
    "data3 = data3.drop('RoomService', axis=1)\n",
    "data3 = data3.drop('FoodCourt', axis=1)\n",
    "data3 = data3.drop('ShoppingMall', axis=1)\n",
    "data3 = data3.drop('Spa', axis=1)\n",
    "data3 = data3.drop('VRDeck', axis=1)\n",
    "\n",
    "\n",
    "data3 = data3.join(bob)\n",
    "\n",
    "mid = data3['Transported']\n",
    "data3.drop(labels=['Transported'], axis=1,inplace = True)\n",
    "data3.insert(0, 'Transported', mid)\n",
    "data3 = data3.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cat =  pd.DataFrame(scaler.fit_transform(data3[['PassengerId', 'age_group', 'FoodCourt_group', 'RoomService_group', 'ShoppingMall_group', 'Spa_group', 'VRDeck_group']]), columns=['PassengerId', 'age_group', 'FoodCourt_group', 'RoomService_group', 'ShoppingMall_group', 'Spa_group', 'VRDeck_group'])\n",
    "data3 = data3.drop(['PassengerId', 'age_group', 'FoodCourt_group', 'RoomService_group', 'ShoppingMall_group', 'Spa_group', 'VRDeck_group'], axis=1)\n",
    "data3 = data3.join(cat)\n",
    "\n",
    "#display(data3.tail(50))\n",
    "\n",
    "X, y = data3.iloc[:, 1:-1], data3.iloc[:, 0]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                           test_size=0.15, \n",
    "                                           shuffle=True,\n",
    "                                           stratify = y,\n",
    "                                           random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8f9f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T23:25:52.557317Z",
     "start_time": "2022-11-13T23:25:52.555447Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# print(len(iris))\n",
    "# print(iris)\n",
    "# # bob = pd.DataFrame(iris.data)\n",
    "# bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafec47d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T23:27:54.473509Z",
     "start_time": "2022-11-13T23:27:54.403897Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_test_split() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d87047e8ec8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train_test_split() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "X, y = train_test_split(X, y,\n",
    "                       test_size=0.10, \n",
    "                       shuffle=True,\n",
    "                       stratify = y,\n",
    "                       random_state=42,\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a447c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-14T00:43:25.143773Z",
     "start_time": "2022-11-14T00:43:25.140750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dominic.mckean/Library/CloudStorage/OneDrive-SecureEngineering/notebooks\n",
      "/opt/anaconda3/envs/test/lib/python37.zip\n",
      "/opt/anaconda3/envs/test/lib/python3.7\n",
      "/opt/anaconda3/envs/test/lib/python3.7/lib-dynload\n",
      "\n",
      "/opt/anaconda3/envs/test/lib/python3.7/site-packages\n",
      "/opt/anaconda3/envs/test/lib/python3.7/site-packages/IPython/extensions\n",
      "/Users/dominic.mckean/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for line in sys.path:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
